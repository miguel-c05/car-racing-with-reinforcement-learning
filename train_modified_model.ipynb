{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f18baea",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00c546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "c:\\Users\\migue\\miniconda3\\envs\\rl-racing\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# Import custom modules\n",
    "from customization import *\n",
    "from learning import Driver\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfe311",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f06c603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Training environments: 2\n",
      "  Total timesteps: 100,000\n",
      "  Save frequency: 10,000\n",
      "  Eval frequency: 10,000\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "NUM_ENVS = 2  # Number of parallel training environments\n",
    "TOTAL_TIMESTEPS = 100_000  # Total training steps\n",
    "SAVE_FREQ = 10_000  # Save checkpoint every N steps\n",
    "EVAL_FREQ = SAVE_FREQ  # Evaluate every N steps\n",
    "N_EVAL_EPISODES = 5  # Number of episodes per evaluation\n",
    "\n",
    "# Directories\n",
    "CHECKPOINT_DIR = \"./modified_models/checkpoints/\"\n",
    "LOG_DIR = \"./logs/modified/\"\n",
    "BEST_MODEL_DIR = \"./modified_models/best_model/\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Training environments: {NUM_ENVS}\")\n",
    "print(f\"  Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"  Save frequency: {SAVE_FREQ:,}\")\n",
    "print(f\"  Eval frequency: {EVAL_FREQ:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73baa2bc",
   "metadata": {},
   "source": [
    "## 3. Create Vectorized Custom Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40befb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 training environments\n",
      "Observation space: Box(0, 255, (84, 96, 3), uint8)\n",
      "Action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n"
     ]
    }
   ],
   "source": [
    "train_env = make_vec_envs(num_envs=NUM_ENVS)\n",
    "\n",
    "print(f\"Created {NUM_ENVS} training environments\")\n",
    "print(f\"Observation space: {train_env.observation_space}\")\n",
    "print(f\"Action space: {train_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2bc49",
   "metadata": {},
   "source": [
    "## 4. Create Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e020df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation environment\n",
      "Eval Observation space: Box(0, 255, (12, 84, 96), uint8)\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "\n",
    "# Create evaluation environment\n",
    "# Usamos a mesma factory, mas precisamos aplicar manualmente os wrappers\n",
    "# que o Driver aplica internamente no treino (Transpose + FrameStack).\n",
    "eval_env = make_vec_envs(num_envs=1)\n",
    "\n",
    "# 1. Transpose from (H, W, C) -> (C, H, W)\n",
    "# Isso coloca os canais de cor no início, padrão PyTorch/SB3\n",
    "eval_env = VecTransposeImage(eval_env)\n",
    "\n",
    "# 2. Stack Frames\n",
    "# O erro \"expected (12, 84, 96)\" indica 4 frames de 3 canais (4*3=12)\n",
    "eval_env = VecFrameStack(eval_env, n_stack=4)\n",
    "\n",
    "print(f\"Created evaluation environment\")\n",
    "print(f\"Eval Observation space: {eval_env.observation_space}\") \n",
    "# Deve imprimir: Box(0, 255, (12, 84, 96), uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39894528",
   "metadata": {},
   "source": [
    "## 5. Initialize Driver with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151cc0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver initialized successfully!\n",
      "Model device: cuda\n",
      "VecEnv has 2 environments (includes FrameStack wrapper)\n"
     ]
    }
   ],
   "source": [
    "# Create the Driver (wraps PPO model with training logic)\n",
    "driver = Driver(\n",
    "    vec_env=train_env,\n",
    "    eval_env=eval_env,\n",
    ")\n",
    "\n",
    "print(f\"Driver initialized successfully!\")\n",
    "print(f\"Model device: {driver.model.device}\")\n",
    "print(f\"VecEnv has {driver.vec_env.num_envs} environments (includes FrameStack wrapper)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53ffbf",
   "metadata": {},
   "source": [
    "## 6. Train the Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6b7d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training for 100,000 timesteps\n",
      "Checkpoints will be saved to: ./modified_models/checkpoints/\n",
      "Best model will be saved to: ./modified_models/best_model/\n",
      "TensorBoard logs: ./logs/modified/\n",
      "\n",
      "To monitor training, run in a terminal:\n",
      "  tensorboard --logdir ./logs/modified/\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10762b1eb5984d6eb22b11689739686c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\anaconda3\\envs\\rl-racing\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001D3912D23F0> != <stable_baselines3.common.vec_env.vec_frame_stack.VecFrameStack object at 0x000001D391645BE0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=10000, episode_reward=25.84 +/- 6.52\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=10000, episode_reward=25.84 +/- 6.52\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 54.60 +/- 2.42\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 54.60 +/- 2.42\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=20000, episode_reward=25.84 +/- 3.24\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=20000, episode_reward=25.84 +/- 3.24\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 52.60 +/- 0.49\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 52.60 +/- 0.49\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=30000, episode_reward=-78.27 +/- 89.58\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=30000, episode_reward=-78.27 +/- 89.58\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 622.80 +/- 461.97\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 622.80 +/- 461.97\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=516.33 +/- 1.12\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=40000, episode_reward=516.33 +/- 1.12\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=50000, episode_reward=516.24 +/- 1.62\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=50000, episode_reward=516.24 +/- 1.62\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=60000, episode_reward=516.16 +/- 0.71\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=60000, episode_reward=516.16 +/- 0.71\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=70000, episode_reward=530.72 +/- 1.72\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=70000, episode_reward=530.72 +/- 1.72\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=531.32 +/- 0.74\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=80000, episode_reward=531.32 +/- 0.74\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=90000, episode_reward=528.90 +/- 3.24\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=90000, episode_reward=528.90 +/- 3.24\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=100000, episode_reward=533.69 +/- 6.90\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=100000, episode_reward=533.69 +/- 6.90\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 1000.00 +/- 0.00\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 1000.00 +/- 0.00\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {TOTAL_TIMESTEPS:,} timesteps\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "print(f\"Best model will be saved to: {BEST_MODEL_DIR}\")\n",
    "print(f\"TensorBoard logs: {LOG_DIR}\")\n",
    "print(\"\\nTo monitor training, run in a terminal:\")\n",
    "print(f\"  tensorboard --logdir {LOG_DIR}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "driver.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521729e2",
   "metadata": {},
   "source": [
    "## 7. Resume Training from Checkpoint (Optional)\n",
    "\n",
    "Use this cell if you want to resume training from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102af664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from latest checkpoint\n",
    "# You can change the number of workers if needed\n",
    "\n",
    "NUM_ENVS_RESUME = 4  # Optional: change number of workers for resumed training\n",
    "\n",
    "driver.resume_training(\n",
    "    target_steps=TOTAL_TIMESTEPS,\n",
    "    num_envs=NUM_ENVS_RESUME  # Set to None to keep same number of envs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca1f05",
   "metadata": {},
   "source": [
    "## 8. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4adc5de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: ./modified_models/best_model//ppo_custom_env_final.zip\n",
      "Final model saved to: ./modified_models/best_model//ppo_custom_env_final.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\anaconda3\\envs\\rl-racing\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'modified_models\\best_model' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "# Save the final trained model\n",
    "final_model_path = f\"{BEST_MODEL_DIR}/ppo_custom_env_final.zip\"\n",
    "driver.save(final_model_path)\n",
    "print(f\"Final model saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead9a7e5",
   "metadata": {},
   "source": [
    "## 9. Close Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f5442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All environments closed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Close all environments\n",
    "train_env.close()\n",
    "eval_env.close()\n",
    "\n",
    "print(\"All environments closed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce3019",
   "metadata": {},
   "source": [
    "## 10. Test the Trained Model (Optional)\n",
    "\n",
    "Visualize the trained agent playing the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d48c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./modified_models/best_model//ppo_custom_env_final.zip\n",
      "\n",
      "Episode 1/3\n",
      "\n",
      "Episode 1/3\n",
      "  Steps: 1000\n",
      "  Total Reward: 527.33\n",
      "  Steps: 1000\n",
      "  Total Reward: 527.33\n",
      "\n",
      "Episode 2/3\n",
      "\n",
      "Episode 2/3\n",
      "  Steps: 1000\n",
      "  Total Reward: 527.37\n",
      "  Steps: 1000\n",
      "  Total Reward: 527.37\n",
      "\n",
      "Episode 3/3\n",
      "\n",
      "Episode 3/3\n",
      "  Steps: 1000\n",
      "  Total Reward: 532.88\n",
      "  Steps: 1000\n",
      "  Total Reward: 532.88\n",
      "Testing complete!\n",
      "Testing complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "\n",
    "# 1. Load the best model\n",
    "best_model_path = f\"{BEST_MODEL_DIR}/ppo_custom_env_final.zip\"\n",
    "# Se der erro de arquivo não encontrado, tente o final:\n",
    "# best_model_path = \"./modified_models/ppo_custom_env_final.zip\"\n",
    "\n",
    "print(f\"Loading model from: {best_model_path}\")\n",
    "model = PPO.load(best_model_path)\n",
    "\n",
    "# 2. Create test env & Apply Wrappers manually\n",
    "test_env = make_vec_envs(num_envs=1)\n",
    "test_env = VecTransposeImage(test_env)\n",
    "test_env = VecFrameStack(test_env, n_stack=4)\n",
    "\n",
    "NUM_TEST_EPISODES = 3\n",
    "\n",
    "try:\n",
    "    for episode in range(NUM_TEST_EPISODES):\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}/{NUM_TEST_EPISODES}\")\n",
    "        \n",
    "        while not done:\n",
    "            # Predict action\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, done_array, info = test_env.step(action)\n",
    "            done = done_array[0] # VecEnv retorna array\n",
    "            total_reward += reward[0]\n",
    "            step_count += 1\n",
    "            \n",
    "            # --- VISUALIZATION LOGIC ---\n",
    "            # Obs shape agora é (1, 12, 84, 96) -> (Batch, Channels, H, W)\n",
    "            \n",
    "            # 1. Pegar o primeiro do batch e Transpor de volta para (H, W, C) para o OpenCV\n",
    "            # shape[0] é 12 (canais). Transpose (1, 2, 0) -> (84, 96, 12)\n",
    "            agent_view = np.transpose(obs[0], (1, 2, 0))\n",
    "            \n",
    "            # 2. Pegar apenas os últimos 3 canais (frame mais recente)\n",
    "            current_frame_rgb = agent_view[:, :, -3:]\n",
    "            \n",
    "            # 3. Converter RGB para BGR\n",
    "            frame_bgr = cv2.cvtColor(current_frame_rgb.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            # 4. Upscale\n",
    "            frame_upscaled = cv2.resize(frame_bgr, (480, 420), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            cv2.imshow(\"Agent View (Stacked)\", frame_upscaled)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                done = True\n",
    "                break\n",
    "        \n",
    "        print(f\"  Steps: {step_count}\")\n",
    "        print(f\"  Total Reward: {total_reward:.2f}\")\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted by user\")\n",
    "finally:\n",
    "    test_env.close()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518093e",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Monitor training with TensorBoard: `tensorboard --logdir ./logs/modified/`\n",
    "- Compare performance with base model (train_base_model.ipynb)\n",
    "- Tune reward hyperparameters in config.py\n",
    "- Analyze the effect of optimal line following\n",
    "- Test different racing tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f18cdc3",
   "metadata": {},
   "source": [
    "## 11. Play with Best Model\n",
    "\n",
    "Load and visualize the best model from training (saved by EvalCallback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36694bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BEST_MODEL_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstable_baselines3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvec_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VecFrameStack, VecTransposeImage\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the best model (saved by EvalCallback during training)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m best_model_path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mBEST_MODEL_DIR\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/ppo_custom_env_final\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading best model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m model = PPO.load(best_model_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'BEST_MODEL_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "\n",
    "# Load the best model (saved by EvalCallback during training)\n",
    "best_model_path = f\"{BEST_MODEL_DIR}/ppo_custom_env_final\"\n",
    "\n",
    "print(f\"Loading best model from: {best_model_path}\")\n",
    "model = PPO.load(best_model_path)\n",
    "\n",
    "# Create test environment with proper wrappers\n",
    "test_env = make_vec_envs(num_envs=1)\n",
    "test_env = VecTransposeImage(test_env)\n",
    "test_env = VecFrameStack(test_env, n_stack=4)\n",
    "\n",
    "NUM_EPISODES = 5\n",
    "\n",
    "print(f\"\\nPlaying {NUM_EPISODES} episodes with best model...\")\n",
    "print(\"Press 'q' to quit\\n\")\n",
    "\n",
    "try:\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        print(f\"Episode {episode + 1}/{NUM_EPISODES}\", end=\" \", flush=True)\n",
    "        \n",
    "        while not done:\n",
    "            # Predict action\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, done_array, info = test_env.step(action)\n",
    "            done = done_array[0]\n",
    "            total_reward += reward[0]\n",
    "            step_count += 1\n",
    "            \n",
    "            # Visualize: obs shape is (1, 12, 84, 96) -> (Batch, Channels, H, W)\n",
    "            # Transpose to (H, W, C) and take last 3 channels (current frame)\n",
    "            agent_view = np.transpose(obs[0], (1, 2, 0))[:, :, -3:]\n",
    "            \n",
    "            # Convert RGB to BGR and upscale\n",
    "            frame_bgr = cv2.cvtColor(agent_view.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "            frame_upscaled = cv2.resize(frame_bgr, (480, 420), interpolation=cv2.INTER_NEAREST)\n",
    "            \n",
    "            cv2.imshow(\"Best Model Playing\", frame_upscaled)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                done = True\n",
    "                break\n",
    "        \n",
    "        print(f\"- Steps: {step_count:4d} | Reward: {total_reward:7.2f}\")\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted by user\")\n",
    "finally:\n",
    "    test_env.close()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nPlayback complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39487ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Videos will be saved to: videos/100000\n",
      "Loading model from: ./modified_models/best_model//ppo_custom_env_final.zip\n",
      "\n",
      "Recording episode 1/3...\n",
      "\n",
      "Recording episode 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\miniconda3\\envs\\rl-racing\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:434: UserWarning: \u001b[33mWARN: Unable to save last video! Did you call close()?\u001b[0m\n",
      "  logger.warn(\"Unable to save last video! Did you call close()?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Steps: 1000 | Reward:  526.18\n",
      "\n",
      "Recording episode 2/3...\n",
      "  Steps: 1000 | Reward:  527.91\n",
      "\n",
      "Recording episode 3/3...\n",
      "  Steps: 1000 | Reward:  527.91\n",
      "\n",
      "Recording episode 3/3...\n",
      "  Steps: 1000 | Reward:  531.06\n",
      "\n",
      "✓ All videos saved to: videos/100000\n",
      "Total episodes recorded: 3\n",
      "  Steps: 1000 | Reward:  531.06\n",
      "\n",
      "✓ All videos saved to: videos/100000\n",
      "Total episodes recorded: 3\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# Create video directory based on training timesteps\n",
    "video_subfolder = f\"videos/{TOTAL_TIMESTEPS}\"\n",
    "os.makedirs(video_subfolder, exist_ok=True)\n",
    "\n",
    "print(f\"Videos will be saved to: {video_subfolder}\")\n",
    "\n",
    "# Load the best model\n",
    "best_model_path = f\"{BEST_MODEL_DIR}/ppo_custom_env_final.zip\"\n",
    "print(f\"Loading model from: {best_model_path}\")\n",
    "model = PPO.load(best_model_path)\n",
    "\n",
    "# Number of episodes to record\n",
    "NUM_RECORD_EPISODES = 3\n",
    "\n",
    "# Record episodes\n",
    "for episode_idx in range(NUM_RECORD_EPISODES):\n",
    "    print(f\"\\nRecording episode {episode_idx + 1}/{NUM_RECORD_EPISODES}...\")\n",
    "    \n",
    "    # Create environment with RecordVideo wrapper\n",
    "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\")\n",
    "    env = CustomEnvironment(env)\n",
    "    \n",
    "    # Add RecordVideo wrapper\n",
    "    env = RecordVideo(\n",
    "        env,\n",
    "        video_folder=video_subfolder,\n",
    "        name_prefix=f\"episode_{episode_idx + 1}\",\n",
    "        episode_trigger=lambda x: True  # Record this episode\n",
    "    )\n",
    "    \n",
    "    # Reset environment\n",
    "    obs, info = env.reset(seed=1000 + episode_idx)\n",
    "    \n",
    "    # Manual Transpose and FrameStack to match training environment\n",
    "    # 1. Transpose (H, W, C) -> (C, H, W)\n",
    "    obs = np.transpose(obs, (2, 0, 1))\n",
    "    \n",
    "    # 2. Stack frames: Create initial stack by repeating the first frame\n",
    "    # Shape becomes (12, 84, 96)\n",
    "    stacked_obs = np.concatenate([obs] * 4, axis=0)\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Predict action\n",
    "        # Model expects (12, 84, 96)\n",
    "        action, _states = model.predict(stacked_obs, deterministic=True)\n",
    "        \n",
    "        # Step environment\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Update frame stack\n",
    "        # 1. Transpose new observation\n",
    "        obs = np.transpose(obs, (2, 0, 1))\n",
    "        \n",
    "        # 2. Shift stack: remove oldest frame (first 3 channels), add new frame (last 3 channels)\n",
    "        stacked_obs = np.concatenate([stacked_obs[3:], obs], axis=0)\n",
    "        \n",
    "        total_reward += reward\n",
    "        step_count += 1\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"  Steps: {step_count:4d} | Reward: {total_reward:7.2f}\")\n",
    "\n",
    "print(f\"\\n✓ All videos saved to: {video_subfolder}\")\n",
    "print(f\"Total episodes recorded: {NUM_RECORD_EPISODES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca783080",
   "metadata": {},
   "source": [
    "## 12. Record Videos of Model Performance\n",
    "\n",
    "Record videos of the trained model playing and save them to the videos folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-racing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
