{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e88334",
   "metadata": {},
   "source": [
    "## Week 1: Customizing the environment\n",
    "\n",
    "### Environment understanding\n",
    "First things first, we took the time to understand the underlying forces at play in this environment. The following are the collected insights:\n",
    "- **Action Space - Discrete vs Continuous**: This environment has, by default, a continuous action space (p.e.: steering $s \\in [-1, 1]$). However, it can be converted to a discrete action space (p.e. by only allowing full left or full right steering).\n",
    "\n",
    "- **Friction**: As per the environment implementation, friction is a vector applied in the oposite direction of the moving car and proportional to its current speed.\n",
    "\n",
    "- **Grip**: Describes the adherence of the car to the track. If the rear wheels angle to the cars current moving direction is too great, especially at high speeds, it will lose grip, and enter a **drifting motion**.\n",
    "\n",
    "Moreover, by reading the environment source code (available under [car_racing.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/car_racing.py) and [car_dynamics.py](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/car_dynamics.py)), we found that:\n",
    "\n",
    "- The track itself is made out of tiles, which are squares with coordinates and rotation variables. They are held in a list in the `CarRacing` class, under `self.track`. Knowing this may make it possible to calculate the line with minimum curvature (ideal trajectory).\n",
    "\n",
    "- Each wheel in `car.wheels` *knows* if it is over one or more road tiles. This is done by checking `len(wheel.tiles)`\n",
    "\n",
    "### Reward modifications\n",
    "We have noticed that the reward function (as described [here](https://gymnasium.farama.org/environments/box2d/car_racing/#rewards)) is very shallow, in the sense that it encourages process (by rewarding per tile completed) but does not encourage any type of behaviour to acheive that progress. As such, we plan to implement the following modifications:\n",
    "\n",
    "- **Gas Bias**: Pushing forward must be encouraged. As such, and to counteract the early training association<br>\n",
    "`moving = crashing`, a small reward will be provided for pushing Gas.\n",
    "\n",
    "- **Wiggle Protection**: Often a model trained on these sorts of environments will perform what is called **intentional wiggling**. That is when, in a curve, for example, the car switches sharply and repeatedly between right-steering and left-steering. This is a technique learnt by models to ensure car grip. It also is, however, a strategy that exploits simple environments, including `CarRacing-v3`, in which steering does not lose speed. To prevent this, sharply changing steering direction will have a moderate penalty.\n",
    "\n",
    "- **Off-road Penalty**: As described below under *Aditional modifications* the simulation will be truncated shortly after the car leaves the track, if it does not return quickly. However, giving a truncation a flat penalty does not seem appropriate, since it gives no context or warning\n",
    "\n",
    "### PID Controller-like reward system\n",
    "In the autonomous racing industry, dynamic error correction is a common theme. The industry standard for such systems is what is called a **PID Controller**. This means taking into account, and trying to correct the error of degree *d* according to its original function (Proportional factor), its derivative  (Differential factor) and its integral (Integral factor). Knowing this the following reward factors were implemented, aditionally to the previous ones:\n",
    "\n",
    "- **Optimal Line Closeness (P factor)**: In a realistic environment, turning, even while accelerating, causes *some* speed loss. Because of this, the optimal line is the one with less curvature, that is, the one where the driver minimizes steering along the whole track. Having that in mind we applied **Laplacian Smoothing** to all tile coordinates, in order to generate that optimal line, and reward the agent for **traversing** the line (**not** for standing still close to it).\n",
    "\n",
    "- **Line Angle Minimization Reward (D factor)**: Velocity is the derivative of position. So, by looking to a cars linear velocity, one can deduce where (and how fast) it will go in the next few moments. Thereby, if the cars current linear velocity vector has a great angle difference to the tracks optimal line, it will deviate very fast, and should correct itself as soon as possible. Because of this, a reward is given to the driver as to encourage angle difference minimization. This avoids the driver **\"Snaking\"** along the track.\n",
    "\n",
    "### Aditional modifications\n",
    "\n",
    "- **Early stopping**: Prevents the simulation from going on when the car has deviated too far out of the track, leaving it aimlessly wandering around. Aims to quicken the training speed.\n",
    "\n",
    "- **HUD removal**: The original box shape (96x96) includes the HUD, displaying bars corresponding to the current action (steering, gas and break). Passing these cluster of pixels effectively represents either redundancy or noise to the model, both of which may impact performance. Consequently, the bottom 12px are cut, leaving the observation space a 84x96 box.\n",
    "\n",
    "- **Frame stacking**: In this environment, there is no visual indication of how fast the car is currently going. As such, for each model prediction (and also in training), a stack of the 4 last frames is passed.\n",
    "\n",
    "---\n",
    "\n",
    "All these implementations can be viewed in [customization.py](customization.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f9092",
   "metadata": {},
   "source": [
    "## Week 2: Reinforcement Learning Agent\n",
    "Having customized the environment to our needs (namely the reward function), we moved on to choosing the reinforcement learning algorithm. We aim to implement many different algorithms and compare them, if time allows it.\n",
    "\n",
    "As both our input and output are **Continuous**, and choosing to implement a On-Policy model, we are left with 3 options: PPO, AC3 and TRPO. We will implement and train the models in this order.\n",
    "\n",
    "### PPO (Proximal Policy Optimization)\n",
    "> **TODO -- Small text explaining overall PPO**\n",
    "\n",
    "#### Starting hyperparameters\n",
    "\n",
    "- `policy = \"CnnPolicy\"`: Since the Car Racing environment observation space is, essentially, a stack of images, a CNN is the most appropriate choice.\n",
    "\n",
    "- `use_sde = True`: (g)SDE or Generalized State-Dependent Exploration is an advanced exploration strategy for Deep Reinforcement Learning (especially algorithms like PPO and A2C) designed to make the agent's actions smoother and more consistent over time. It replaces the standard \"random jitter\" noise with a **\"structured\" noise** that depends on the state of the environment. This helps in environments that mimic physics related problems by \"directing\" its exploration consistently throughout an episode, avoiding rapidly changing inputs in instances where only longer, consistent ones have measurable outcomes.\n",
    "\n",
    "> **TODO -- complete hyperparameter description and explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a599d44",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-racing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
